{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Basically, a Bayesian Neural Network gives several different predictions for the same data point by neural networks with different values for the weights and biases, and finally, we get a mean and standard deviation of all these predictions for that data point to plot the confidence interval. No changes were made from the original architecture and implementation of these networks as published by the respective authors in GitHub. For details about the **architecture** of these networks please see **References** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "Three experiments were carried out with different combinations for training and test data. All 3 versions (v1, v2 and v3) contain predictions on data points for TZ6 LTR dataset, but varies in each version is the dataset used as _training and test data points_.\n",
    "* v1 - uses some 1080 points from FDDN for training the BNN and then predictions on made on all 34 LTR data points.\n",
    "* v2 - contains training data from LTR (25 points) used to train the BNN for prediction on some 8 LTR data points.\n",
    "* v3 - uses some 1995 points from the Artificially Synthesised Data close to the LTR data distribution were used for training the BNN and then predictions on made on all the original 34 LTR data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Hypothesis</font> & Observations\n",
    "1. <font color='blue'> Does any particular BNN architecture work faster out of the box? </font>\n",
    "    <br>`MC DropOut` typically works faster compared to `BNN Ensembling`. The training process completes in ~ 20 secs for MC DropOut as opposed to ~ 260 secs for Bayesian Ensembling. One main reason for this is the no. of neural networks we use for the BNN Ensemble (typically 10), where each neural network has to be trained with the entire training dataset for the set nr. of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <font color='blue'> Are predictions by the BNN architectures close to the true values? </font> \n",
    "    <br>MC DropOut seems to give mean values of predictions which are much closer to the true values compared to Bayesian Ensembling, even in the case experiment v1 where the simulation data distribution is inherently different compared to the LTR data distribution. The figure below shows the prediction results on LTR data points for *v1* version of the experiment where the BNN networks were trained with simulated data points from FDDN Solver (1080 points) and then the neural networks were used to make the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><b><center>BNN Ensembling (Experiment - v1)</center></b><img src='images/BNN_Ensembling_Image1_ClosetoPredictions.png'></td>\n",
    "        <td><b><center>MC DropOut (Experiment - v1)</center></b><img src='images/MC_DropOut_Image1_ClosetoPredictions.png'></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. <font color='blue'> Does the nature of the training data distribution impact the accuracy of the predictions? </font>     \n",
    "    When the training data distribution is close to the test data points (LTR) and training data size is sufficiently large, we see more accurate predictions with tighter confidence intervals as in `v3`. Bayesian Ensembling seems to be giving highly confident predictions in this case.\n",
    "    <table>\n",
    "    <tr>\n",
    "        <td><b><center>BNN Ensembling (Experiment - v3)</center></b><img src='images/BNN_Ensembling_Image2_Expv3.png'></td>\n",
    "        <td><b><center>MC DropOut (Experiment - v3)</center></b><img src='images/MC_DropOut_Image2_Expv3.png'></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <font color='blue'> Do the confidence intervals differ for predictions made by the two different BNN architectures? </font>    \n",
    "    Bayesian Ensembling technique generally seems to give results with minimal deviation from the mean (i.e confidence interval whiskers are tighter for most predictions). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planned Tasks\n",
    "* The results ( individual data points) have to be looked at more closely to ascertain errors in the dataset (if any) and the nature of the predictive distribution.\n",
    "* HyperParameters Tuning for the two architectures needs to be investigated further to ascertain their impact on the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. Bayesian Ensembling [GitHub](https://github.com/TeaPearce/Bayesian_NN_Ensembles) | [arXiv](https://arxiv.org/abs/1810.05546)\n",
    "2. MC DropOut [GitHub](https://github.com/yaringal/DropoutUncertaintyExps/blob/master/net/net.py) | [arXiv](https://arxiv.org/abs/1506.02142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
